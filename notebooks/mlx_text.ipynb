{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52e4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f822ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5317a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-albert-small-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "from raggler.rag import create_index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from raggler.rag import RAG\n",
    "\n",
    "from raggler.indexes import NPIndex\n",
    "from raggler.mlx_llm import MLXLLM\n",
    "from raggler.llm_context import RAG_TEMPLATE\n",
    "\n",
    "\n",
    "model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "\n",
    "try: \n",
    "    index = NPIndex()\n",
    "    index.load('../data/indexes/')\n",
    "except Exception:\n",
    "    index  = create_index(['/Users/kostis/Library/Mobile Documents/iCloud~md~obsidian/Documents'], embedder=model, path_to_save_index='../tests/test_index/')\n",
    "   # index.save('../data/indexes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6a38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-albert-small-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:raggler.mlx_llm:Loading model from data/models/mlx-community/NeuralBeagle14-7B-4bit-mlx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SentenceTransformer('paraphrase-albert-small-v2')\n",
    "\n",
    "test = MLXLLM(\"mlx-community/NeuralBeagle14-7B-4bit-mlx\", prompt_template=RAG_TEMPLATE)\n",
    "\n",
    "\n",
    "rag = RAG(embedder=model, index = index, language_model=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d88556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4fdc1b324246a6b9998aee0e5ad69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "CONTEXT:\n",
      "\n",
      "At its core, CUPED is a proposal for how to use pre-experiment data with control variates to reduce the variance of the ATE estimator. The authors propose using any covariates that we have before the experiment took place, $X_i$, $i=1,\\ldots, n$, as well as past values of $Y$ to fit a linear model:\n",
      "$$\\hat{Y}=X\\beta +\\epsilon$$ You can find implementations of [CUPED and CUPAC in this notebook](https://github.com/kgourgou/cupac_cuped_control_variates/blob/main/cuped_cupac.ipynb).\n",
      "\n",
      "CUPED stands for \"Controlled-Experiment using Pre-experiment data\"; see Deng, Xu, Kohavi, Walker, 2013.\n",
      "\n",
      "Question: Explain CUPED please.\n",
      "\n",
      "Answer:\n",
      "\n",
      "CUDED, or CUPED (Controlled-Experiment using Pre-experiment Data), is a method proposed in 2013 by Deng, Xu, Kohavi, and Walker to reduce the variance of the Average Treatment Effect (ATE) estimator in an experiment. It does this by utilizing pre-existing data, specifically covariates ($X_i$) and past values of the outcome variable ($Y$) before the experiment began.\n",
      "\n",
      "The approach involves fitting a linear model to predict the outcome variable ($Y$) using the available pre-experiment data:\n",
      "$$\\hat{Y} = X\\beta + \\epsilon$$\n",
      "\n",
      "This model,"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExplain CUPED please.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/raggler_dev/raggler/rag.py:143\u001b[0m, in \u001b[0;36mRAG.__call__\u001b[0;34m(self, query, k, show_context)\u001b[0m\n\u001b[1;32m    140\u001b[0m     sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/raggler_dev/raggler/mlx_llm.py:72\u001b[0m, in \u001b[0;36mMLXLLM.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mQuery the language model with the given text.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    str: The response from the language model.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mDEFAULT_OPTIONS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/mlx_lm/utils.py:258\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompt, temp, max_tokens, verbose, formatter, repetition_penalty, repetition_context_size, top_p)\u001b[0m\n\u001b[1;32m    256\u001b[0m     prompt_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m    257\u001b[0m     tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 258\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    261\u001b[0m     s \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rag('Explain CUPED please.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321d399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
